---
title: "Week6"
author: "Paul Hewson"
date: "16/03/2021"
output: html_document
---

```{r setup, include=FALSE}
library(ggplot2)
library(dplyr)
library(readr)
# Helper functions, please ignore (treat as if they were in a package)
ci_upper <- function(ci_percent){
  one_minus_alpha = ci_percent / 100
  alpha <- 1 - one_minus_alpha
  return(1 - alpha / 2)
}

get_cox_ci <- function(y, upper=TRUE, ci_percent=95){
  add_or_subtract = (upper * 2) - 1
  x <- log(y)
  s2 <- var(x)
  n <- length(x)
  nu <- qt(ci_upper(ci_percent), (n-1))
  halfwidth <- nu  * sqrt((s2/n) + (s2^2/(2 * (n - 1))))
  correction <- s2 / 2
  limit <- mean(x) + correction + add_or_subtract * halfwidth
  return(exp(c(limit)))
}

y <- c(914.9, 1568.3, 50.5, 94.1, 199.5, 23.8, 70.5, 213.1,
       44.1, 331.7, 139.3, 115.6, 38.4, 357.1, 725.9,	253.2,
       905.6,	155.4, 138.1,	95.2,	75.2,	275.0, 401.1,	653.8,
       390.8, 483.5, 62.6, 128.5, 81.5, 218.5, 308.2,	41.2,
       60.3, 506.9, 221.8, 112.5, 93.7, 199.3, 210.6,	39.2)
test_lower <- get_cox_ci(y, FALSE)
test_upper <- get_cox_ci(y)
test_lower -  188.0608 < 0.0001
test_upper - 414.5752 < 0.0001


standard_error_of_mean <- function(data){
  return(sd(data) / sqrt(length(data)))
}

conf_width <- function(data, ci_percent=95){
  one_minus_alpha = ci_percent / 100
  alpha <- 1 - one_minus_alpha
  quantile = (1 - alpha / 2)
    return(qnorm(quantile) * standard_error_of_mean(data))  
}

confint_to_df <- function(ci_matrix){
    return(data.frame(
      names = names(ci_matrix[,1]), 
      lower = ci_matrix[,1],
      upper = ci_matrix[,2]))
}

prediction_intervals <- function(model, data_x, level=0.95){
    preds <- predict(model, level=level,
                     newdata=data.frame(x=levels(data_x)),
                     se.fit=TRUE, interval="prediction")
    return(data.frame(lower_limit = preds$fit[,2],
                      fitted_value = preds$fit[,1],
                      upper_limit = preds$fit[,3]))
}

lower_ci <- NA
upper_ci <- NA

```

## A generic rule of thumb for a 95% Confidence Interval

$$\hat{\theta} \pm 2 se(\theta)$$

This rule of thumb is worth memorising. It can be really useful, whether working with written reports or when wanting a quick check on some data analysis.


## Computing CIs: an example with fake data

Repeating the example from the presentation


```{r simple_fake_data}
y <- rnorm(52, 50, 5)
fake <- data.frame(y=y)
fake %>%
  ggplot(aes(y)) +
  geom_histogram(binwidth = 5)
```


To rehearse the key idea, we will calculate a 95 percent confidence interval.

$$\bar{x} \pm 2 \times \mbox{standard error of the mean}$$

where 

$$\mbox{standard error of the mean} = \frac{sd}{\sqrt{n}}$$

### Exercise 1: Complete the code to calculate the confidence limits

library(readr)
fake <- read_csv("data/ks-projects-201801.csv")
View(fake)

```{r, calculate_ci_manually}
t_value <- qt(0.975, 51)
fake_mean <- mean(fake$y)
fake_sd <- sd(fake$y)
n <- length(fake$y)
fake_se_mean <- fake_sd/sqrt(n)
lower_ci <- fake_mean-(2*fake_se_mean)
upper_ci <- fake_mean +(2*fake_se_mean)
sprintf("95 percent CI for the mean: %f - %f", lower_ci, upper_ci)
```

Compare your results with those you get from the R modelling routines:

```{r CI from model}
model_fake <- lm(y ~ 1, data = fake)
confint(model_fake, level=0.95)
```

Any comments or observations on the "hand" calculated 95% CI and the model calculated 95% CI?

A: observed and actual values closely match

The kickstarter data
---------------------------

We asked you to pin datasets you find interesting on a map. One of you pinned some Kaggle based Kickstarter data.   All the information and metadata can be found by following the pin at <https://padlet.com/texhewson/zp5pj3hj5xulscxe> (it's the pin on the border between Wyoming and South Dakota)

In this example, I have downloaded the data to a folder adjacent to my working folder called data.

```{asis}
---CourseFolder
       |
       +------- scripts (My working folder, contains my Rmd files)
       |
       +------- data (Contains data files)
```

So when I load the file into R, I have to tell it where to look for the file.  `../` tells it to go up one folder.   `data/` tells it to look in the data folder.  I have also used a function from the `readr` library (called `read_csv`) which gives me a lot more control over how I load the data.  If you click on a file in the Files pane of R studio, it offers to load the data using this function. But here the command has been captured from the console and scripted.


```{r load_kickstarter, echo=FALSE}
kickstarter <- read_csv(
  "../data/ks-projects-201801.csv", 
  col_types = cols(
    currency = col_factor(levels = c(
      "AUD",  "CAD", "CHF", "DKK", "EUR", "GBP", "HKD", "JPY",
      "MXN", "NOK", "NZD", "SEK", "SGD", "USD")), 
    state = col_factor(levels = c(
      "canceled",  "failed", "live", "successful", "suspended",
      "undefined")),
    country = col_factor(levels = c(
      "AT", "AU", "BE", "CA", "CH", "DE", "DK", "ES", "FR", "GB",
      "HK", "IE", "IT", "JP", "LU", "MX", "N,0\"", "NL", "NO",
      "NZ", "SE", "SG", "US")),
    main_category = col_factor(levels =c (
      "Art", "Comics", "Crafts", "Dance", "Design", "Fashion",
      "Film & Video", "Food", "Games", "Journalism", "Music", 
      "Photography", "Publishing", "Technology", "Theater")),
    deadline = col_date(format = "%Y-%m-%d"), 
    launched = col_datetime(format = "%Y-%m-%d %H:%M:%S")
    ))
```

This is a large dataset. It may speed things up if you take a random subsample.  The following code takes a random sample of 30,000 rows from the data, and then deletes the large dataset.  Feel free to reduce the sample size if your computer needs it.

```{r take_sample}
n_subsample <- 30000
index <- sample(c(1:length(kickstarter$ID)), n_subsample)
my_kickstarter <- kickstarter[index, ]
rm(kickstarter)
```

## Looking at the number of backers by state of project

As an example to get us started, here is a boxplot of the number of funders by the state of each project.  You might like to experiment with adding and removing the `scale_y_log10()` instruction. The problem with using it is that it gets confused by all the 0 funders (the value $\log_{10}(0)$ is undefined).  If you remove this instruction, you will see another problem. Can you think of any workrounds?

```{r backers, echo=FALSE}
my_kickstarter %>%
  ggplot(aes(x=state, y=backers)) + 
  geom_boxplot() + 
  scale_y_log10() +
  labs(title = "Number of backers by state of project",
       subtitle = "Kickstarter enabled funding calls",
       caption = "Data from Kickstarter via Kaggle",
       tag = "Figure 1") + 
  xlab("State of project") + 
  ylab("Number of backers") + 
  theme_bw()  
```

Exericse 2a: Improve the visual representation of the number of funders
------------------------------------------------------------------------------

I would suggest this is challenging data to visualise well.  But it could be worth getting to grips with this because the implication is that `succesful` projects tend to have more backers than `failed` projects.


## Looking at the funding goal by the main category

To look at some confidence intervals, we will consider the funding goal variable.  We will first see how the distribution of funding goals varies by the main category of the funding call.

```{r goal_by_project, echo=FALSE}
my_kickstarter %>%
  ggplot(aes(x=main_category, y=usd_goal_real)) + 
  geom_boxplot() + 
  scale_y_log10() +
  coord_flip() +
  labs(title = "Size of Kickstarter goal (USD) by type of project",
       subtitle = "Kickstarter enabled funding calls",
       caption = "Data from Kickstarter via Kaggle",
       tag = "Figure 2") + 
  xlab("Type of project") + 
  ylab("Funding Goal (USD)") + 
  theme_bw()  
```

Exercise 2b: Improvide the visualisation of funding goal by main category
-----------------------------------------------------------------------------

Again, we have had to use a logarithmic transform of the $y$ axis. There were no problems with $0$ (who starts a kickstarter requesting $0$ funding?). But it might play down the differences. Technology projects for example seem to have much larger goals on averate than Dance or Theatre.  Try this visual with and without `scale_y_log_10()`.   Do you have any other suggestions.




### Look at each main category separately

First, look at the funding goals for technology projects

```{r goal_us, echo=FALSE}
my_kickstarter %>%
  filter(main_category %in% c("Technology")) %>%
  ggplot(aes(x=usd_goal_real)) + 
  geom_histogram(aes(y =..density..)) +
  scale_x_log10() +
  labs(title = "Distribution of funding goals for Tech projects",
       subtitle = "Kickstarter enabled funding calls",
       caption = "Data from Kickstarter via Kaggle",
       tag = "Figure 4") + 
  xlab("Funding Goal (USD)") + 
  ylab("Density") + 
  theme_bw()  
```


First we can use the "hand calculate" method for computing the 95% for funding goals for technology


```{r conf_int_us}
my_kickstarter %>%
  filter(main_category %in% c("Technology")) %>%
  summarise(
    lci=mean(usd_goal_real) - conf_width(usd_goal_real),
    mean=mean(usd_goal_real), 
    uci=mean(usd_goal_real) + conf_width(usd_goal_real)
    )
```

Alternatively, we can use the modelling function:


```{r ci_model_us}
model <- lm(usd_goal_real ~ 1, data = my_kickstarter,
            subset=main_category == "Technology")
confint(model)

```
Exercise 3a
-------------

How do these two methods for computing the 95% CI compare?


## Using a model to obtain 95% CI for the population mean for each main category

We will cover modelling in detail in a few weeks.  For now, just note that we have to set the intercept to -1 along with setting a group variable (a factor) so that we only get estimates for each group mean.

```{r ci_goal_global}
model <- lm(usd_goal_real ~ -1 + main_category, data = my_kickstarter)
cis <- confint(model)
cis

```

Exercise 3b: Compare the model fitted to all groups with one group at a time
-----------------------------------------------------------------------------

- How does the 95% CI for Technology compare when we use a linear model to fit a CI to all group means, and when we use a linear model to fit a CI to Technology on its own?
- Do you see any other problems in these Confidence Intervals

Using logarithms and plotting the confidence intervals
=======================================================

We are going to see if we can work with $\log_{10}(\mbox{Funding Goal})$ instead of the untransformed data.

First, we can obtain group means, on a $\log_{10}$ scale:

```{r}
group_means <- my_kickstarter %>%
  group_by(main_category) %>%
  summarise(mean_log = mean(log10(usd_goal_real)))
group_means
```

Next we need to fit a model to the $\log_{10}$ transformed data as well.   We can then plot these results.  

```{r plot_cis}
model_log <- lm(log10(usd_goal_real) ~ -1 + main_category, data = my_kickstarter)
cis <- confint(model_log)
cis
ci <- confint_to_df(cis)

ci %>% ggplot(aes(x=lower, y = names)) +
  geom_segment(aes(x=lower, xend=upper, y=names, yend=names), color="grey") +
  geom_point( aes(x=lower, y=names), color=rgb(0.2,0.7,0.1,0.5), size=3 ) + 
  geom_point(aes(x=upper, y=names), color=rgb(0.2,0.7,0.1,0.5), size=3 ) +
  geom_vline(xintercept = group_means$mean_log, color="lightblue", 
             lty=2, lwd=1) +
  theme_bw()  
```

The code line `geom_vline(...)` adds a vertical line for the group means.  Perhaps this could be improved?  Could you select a few group means that are of most interest?  Could you change the colour/line style and add a legend which tells us which group mean is displayed?


These look like better behaved confidence intervals. We can certainly make a comparison as to which groups overlap which other group.However, they only work on the $\log_{10}$ data.    We just aren't working in units that make sense to most business people interpreting these results.   

Niche result
=======================

In this case, we *could* do a literature search and use a modified Cox method to give us better behaved confidence intervals.  For reference, the papers describing this work are:

- Land, C. E. (1971), “Confidence intervals for linear functions of the normal mean and variance,” Annals of Mathematical Statistics, 42, 1187-1205.
- Zhou, X-H., and Gao, S. (1997), “Confidence intervals for the log-normal mean,” Statistics in Medicine, 16, 783-790. 

The formula for the method can be given as:

$\bar{x} + \frac{S^2}{2} \pm t_{(1-\alpha), \nu} \sqrt{\frac{S^2}{n} + \frac{S^4}{2(n-1)}}$

where $\bar{x}$ is the sample mean, $S^2$ is the sample variance, $n$ is the sample size and $\nu$ denotes the degrees of freedom of the t-distribution.   We have provided R functions to implement this method.


```{r}
cox_ci <- my_kickstarter %>%
  group_by(main_category) %>%
  summarise(lower=get_cox_ci(usd_goal_real, FALSE), upper=get_cox_ci(usd_goal_real, TRUE))

cox_ci %>% ggplot(aes(x=lower, y = main_category)) +
  geom_segment(aes(x=lower, xend=upper, y=main_category, yend=main_category), color="grey") +
  geom_point( aes(x=lower, y=main_category), color=rgb(0.2,0.7,0.1,0.5), size=3 ) + 
  geom_point(aes(x=upper, y=main_category), color=rgb(0.2,0.7,0.1,0.5), size=3 ) +
  theme_bw()  
```

Exercise 4
------------


- Do these "Cox" derived confidence intervals appear rational and believable
- Are the results interpretable by a business person analysing kickstarter funding
- These results are based on computing a group mean at a time, independent of other group means.   What do you think of that in the light of your answers to Exercise 2b.


Exercise 5 and Homework
========================


Create one more grouped scenario and calculate the confidence intervals.

- You could consider a different grouping variable, a different response variable or both.  
- You could also conduct an analysis on a sensible subset of the data, for example examine only those kickstarter projects that were successful.
- Before you calculate the group mean confidence intervals, make sure you do appropriate exploratory data analysis and ensure the data quality procedures and meta data are completed.



Summary
==========


- Don't panic that we are going to expect you to learn many methods for creating "good" confidence intervals
- Do note that on real data, sometimes the textbook methods don't work and you need to be able to call this out.
- This week, concentrate on the learning points. What is a confidence interval e.g., what should it tell us, why use a 90%, 95% or 99% interval.
- Next week, we will look at the "Bootstrap" which provides a computationally intensive, but very general, way of computing a general purpose confidence interval that should behave in most circumstances.

